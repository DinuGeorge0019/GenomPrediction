{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the global seed\n",
    "_GLOBAL_SEED = 42\n",
    "random.seed(_GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the training dataset\n",
    "train_df = pd.read_csv('../_Dataset/train_dataset.csv')\n",
    "\n",
    "# Read the test dataset\n",
    "test_df = pd.read_csv('../_Dataset/test_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genetic_disorder_df = train_df.drop(\"disorder_subclass\", axis=1)\n",
    "train_disorder_subclass_df = train_df\n",
    "\n",
    "test_disorder_subclass_df = test_df.drop(\"genetic_disorder\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genetic_disorder_x = train_genetic_disorder_df.drop(\"genetic_disorder\",axis=1)\n",
    "train_genetic_disorder_y = train_genetic_disorder_df[\"genetic_disorder\"]\n",
    "\n",
    "train_disorder_subclass_x = train_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "train_disorder_subclass_y = train_disorder_subclass_df[\"disorder_subclass\"]\n",
    "\n",
    "test_disorder_subclass_x = test_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "test_disorder_subclass_y = test_disorder_subclass_df[\"disorder_subclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Genetic Disorder training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "train_genetic_disorder_x = scaler.fit_transform(train_genetic_disorder_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best model for identifing the genetic disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CalibratedClassifierCV(cv=5,\n",
       "                       estimator=&lt;catboost.core.CatBoostClassifier object at 0x000001E8C5EBF950&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CalibratedClassifierCV</label><div class=\"sk-toggleable__content\"><pre>CalibratedClassifierCV(cv=5,\n",
       "                       estimator=&lt;catboost.core.CatBoostClassifier object at 0x000001E8C5EBF950&gt;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x000001E8C5EBF950&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x000001E8C5EBF950&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "CalibratedClassifierCV(cv=5,\n",
       "                       estimator=<catboost.core.CatBoostClassifier object at 0x000001E8C5EBF950>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catb_classifier = CatBoostClassifier(random_state=_GLOBAL_SEED, verbose=False)\n",
    "\n",
    "# Assuming catb_classifier is your CatBoostClassifier\n",
    "catb_classifier = CalibratedClassifierCV(catb_classifier, method='sigmoid', cv=5)\n",
    "\n",
    "# Train the classifier\n",
    "catb_classifier.fit(train_genetic_disorder_x, train_genetic_disorder_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the genetic disorder for the test dataset using the genetic disorder classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genetic_disorder_predictions = catb_classifier.predict(test_disorder_subclass_x)\n",
    "\n",
    "test_disorder_subclass_x['genetic_disorder'] = test_genetic_disorder_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "train_disorder_subclass_x = scaler.fit_transform(train_disorder_subclass_x)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "test_disorder_subclass_x = scaler.transform(test_disorder_subclass_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(solver='newton-cg', random_state=_GLOBAL_SEED)\n",
    "kn_classifier = KNeighborsClassifier()\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=_GLOBAL_SEED)\n",
    "gaussian_nb_classifier = GaussianNB()\n",
    "random_forest_classifier = RandomForestClassifier(random_state=_GLOBAL_SEED)\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(random_state=_GLOBAL_SEED)\n",
    "xgb_classifier = XGBClassifier(random_state=_GLOBAL_SEED)\n",
    "lgb_classifier = LGBMClassifier(random_state=_GLOBAL_SEED)\n",
    "svc_classifier = SVC(decision_function_shape='ovo')\n",
    "catb_classifier = CatBoostClassifier(random_state=_GLOBAL_SEED, verbose=False)\n",
    "\n",
    "# Assuming catb_classifier is your CatBoostClassifier\n",
    "catb_classifier = CalibratedClassifierCV(catb_classifier, method='sigmoid', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models on the genetic disorder training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 724\n",
      "[LightGBM] [Info] Number of data points in the train set: 14437, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -4.886202\n",
      "[LightGBM] [Info] Start training from score -5.287090\n",
      "[LightGBM] [Info] Start training from score -1.741575\n",
      "[LightGBM] [Info] Start training from score -2.375633\n",
      "[LightGBM] [Info] Start training from score -2.699223\n",
      "[LightGBM] [Info] Start training from score -3.439823\n",
      "[LightGBM] [Info] Start training from score -1.350441\n",
      "[LightGBM] [Info] Start training from score -1.522392\n",
      "[LightGBM] [Info] Start training from score -1.948060\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "kn_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "decision_tree_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "gaussian_nb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "random_forest_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "gradient_boosting_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "xgb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "lgb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "svc_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "catb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "\n",
    "models_collection = {\n",
    "    'Logistic Regression': logistic_regression_classifier, \n",
    "    'K-Nearest Neighbors': kn_classifier, \n",
    "    'Decision Tree': decision_tree_classifier, \n",
    "    'Gaussian Naive Bayes': gaussian_nb_classifier, \n",
    "    'Random Forest': random_forest_classifier, \n",
    "    'Gradient Boosting': gradient_boosting_classifier, \n",
    "    'XGBoost': xgb_classifier, \n",
    "    'LightGBM': lgb_classifier,\n",
    "    'Support Vector Classifier': svc_classifier,\n",
    "    'CatBoost': catb_classifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_collection.items():\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict(test_disorder_subclass_x)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_disorder_subclass_y, predictions)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = recall_score(test_disorder_subclass_y, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(test_disorder_subclass_y, predictions)\n",
    "    \n",
    "    # Store the results\n",
    "    results[model_name] = {'Accuracy': accuracy, 'Recall': recall, 'Mean Squared Error': mse}\n",
    "    \n",
    "# Sort the results by accuracy\n",
    "sorted_results = sorted(results.items(), key=lambda item: item[1]['Accuracy'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.2897506925207756\n",
      "Recall: 0.2897506925207756\n",
      "Mean Squared Error: 5.665096952908587\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: CatBoost\n",
      "Accuracy: 0.2883656509695291\n",
      "Recall: 0.2883656509695291\n",
      "Mean Squared Error: 5.984764542936288\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: XGBoost\n",
      "Accuracy: 0.28753462603878116\n",
      "Recall: 0.28753462603878116\n",
      "Mean Squared Error: 5.712188365650969\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: LightGBM\n",
      "Accuracy: 0.2867036011080332\n",
      "Recall: 0.2867036011080332\n",
      "Mean Squared Error: 6.002493074792244\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.2739612188365651\n",
      "Recall: 0.2739612188365651\n",
      "Mean Squared Error: 6.424376731301939\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.26204986149584486\n",
      "Recall: 0.26204986149584486\n",
      "Mean Squared Error: 8.148476454293629\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.2554016620498615\n",
      "Recall: 0.2554016620498615\n",
      "Mean Squared Error: 7.11218836565097\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.25041551246537397\n",
      "Recall: 0.25041551246537397\n",
      "Mean Squared Error: 6.814681440443214\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Support Vector Classifier\n",
      "Accuracy: 0.25041551246537397\n",
      "Recall: 0.25041551246537397\n",
      "Mean Squared Error: 6.621329639889197\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Gaussian Naive Bayes\n",
      "Accuracy: 0.24265927977839336\n",
      "Recall: 0.24265927977839336\n",
      "Mean Squared Error: 7.053462603878116\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the sorted results\n",
    "for model_name, metrics in sorted_results:\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the top 4 models to create an ensemle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 724\n",
      "[LightGBM] [Info] Number of data points in the train set: 14437, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -4.886202\n",
      "[LightGBM] [Info] Start training from score -5.287090\n",
      "[LightGBM] [Info] Start training from score -1.741575\n",
      "[LightGBM] [Info] Start training from score -2.375633\n",
      "[LightGBM] [Info] Start training from score -2.699223\n",
      "[LightGBM] [Info] Start training from score -3.439823\n",
      "[LightGBM] [Info] Start training from score -1.350441\n",
      "[LightGBM] [Info] Start training from score -1.522392\n",
      "[LightGBM] [Info] Start training from score -1.948060\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Ensemble accuracy: 0.29085872576177285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Get the top 4 models\n",
    "top_4_models = sorted_results[:4]\n",
    "\n",
    "# Create a list of tuples where each tuple contains a model name and an estimator\n",
    "estimators = [(model_name, models_collection[model_name]) for model_name, _ in top_4_models]\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "ensemble_predictions = ensemble.predict(test_disorder_subclass_x)\n",
    "\n",
    "# Calculate and print the accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(test_disorder_subclass_y, ensemble_predictions)\n",
    "print(f\"Ensemble accuracy: {ensemble_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genom_prediction_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
