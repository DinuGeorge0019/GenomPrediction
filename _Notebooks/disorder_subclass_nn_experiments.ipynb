{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\scoala\\master anul 1\\Machine Learning\\GenomPrediction\\genom_prediction_venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the global seed\n",
    "_GLOBAL_SEED = 42\n",
    "random.seed(_GLOBAL_SEED)\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(_GLOBAL_SEED)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(_GLOBAL_SEED)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(_GLOBAL_SEED)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(_GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the training dataset\n",
    "train_df = pd.read_csv('../_Dataset/train_dataset.csv')\n",
    "\n",
    "# Read the test dataset\n",
    "test_df = pd.read_csv('../_Dataset/test_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_disorder_subclass_df = train_df.drop(\"genetic_disorder\", axis=1)\n",
    "test_disorder_subclass_df = test_df.drop(\"genetic_disorder\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_disorder_subclass_x = train_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "train_disorder_subclass_y = train_disorder_subclass_df[\"disorder_subclass\"]\n",
    "\n",
    "test_disorder_subclass_x = test_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "test_disorder_subclass_y = test_disorder_subclass_df[\"disorder_subclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "train_disorder_subclass_x = scaler.fit_transform(train_disorder_subclass_x)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "test_disorder_subclass_x = scaler.transform(test_disorder_subclass_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_disorder_subclass_x, val_disorder_subclass_x, train_disorder_subclass_y, val_disorder_subclass_y = train_test_split(train_disorder_subclass_x, train_disorder_subclass_y, test_size=0.1, random_state=_GLOBAL_SEED)\n",
    "\n",
    "train_disorder_subclass_y = to_categorical(train_disorder_subclass_y)\n",
    "val_disorder_subclass_y = to_categorical(val_disorder_subclass_y)\n",
    "test_disorder_subclass_y = to_categorical(test_disorder_subclass_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "_NUM_CLASSES = 9\n",
    "_TOTAL_EPOCHES = 10\n",
    "\n",
    "# Define a function that creates a model\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=train_disorder_subclass_x.shape[1], activation='relu'))  # First layer with number of neurons equal to number of input features\n",
    "    for i in range(hp.Int('num_layers', 1, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=256,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(_NUM_CLASSES, activation='softmax'))  # _NUM_CLASSES is the number of classes\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [2e-2, 2e-3, 2e-4])), # , 2e-2, 2e-3, 2e-4\n",
    "        loss='categorical_crossentropy',  # or 'sparse_categorical_crossentropy'\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < int(0.2 * _TOTAL_EPOCHES):  # 10% of total epochs\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)  # decrease the learning rate\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start parameters tunning and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 27s]\n",
      "val_accuracy: 0.4219759901364644\n",
      "\n",
      "Best val_accuracy So Far: 0.4219759901364644\n",
      "Total elapsed time: 00h 09m 12s\n",
      "Epoch 1/10\n",
      "407/407 [==============================] - 7s 7ms/step - loss: 1.4973 - accuracy: 0.3593 - val_loss: 1.4193 - val_accuracy: 0.3871\n",
      "Epoch 2/10\n",
      "407/407 [==============================] - 3s 7ms/step - loss: 1.3702 - accuracy: 0.3971 - val_loss: 1.3742 - val_accuracy: 0.4058\n",
      "Epoch 3/10\n",
      "407/407 [==============================] - 3s 6ms/step - loss: 1.3423 - accuracy: 0.4125 - val_loss: 1.3726 - val_accuracy: 0.4120\n",
      "Epoch 4/10\n",
      "407/407 [==============================] - 2s 6ms/step - loss: 1.3313 - accuracy: 0.4175 - val_loss: 1.3550 - val_accuracy: 0.4114\n",
      "Epoch 5/10\n",
      "407/407 [==============================] - 2s 6ms/step - loss: 1.3259 - accuracy: 0.4164 - val_loss: 1.3628 - val_accuracy: 0.3947\n",
      "Epoch 6/10\n",
      "407/407 [==============================] - 3s 7ms/step - loss: 1.3135 - accuracy: 0.4195 - val_loss: 1.3668 - val_accuracy: 0.4217\n",
      "Epoch 7/10\n",
      "407/407 [==============================] - 3s 6ms/step - loss: 1.3008 - accuracy: 0.4249 - val_loss: 1.3662 - val_accuracy: 0.4065\n",
      "Epoch 8/10\n",
      "407/407 [==============================] - 3s 6ms/step - loss: 1.2941 - accuracy: 0.4252 - val_loss: 1.3879 - val_accuracy: 0.4003\n",
      "Epoch 9/10\n",
      "407/407 [==============================] - 3s 6ms/step - loss: 1.2832 - accuracy: 0.4338 - val_loss: 1.3937 - val_accuracy: 0.4197\n",
      "Epoch 10/10\n",
      "407/407 [==============================] - 2s 5ms/step - loss: 1.2758 - accuracy: 0.4355 - val_loss: 1.4000 - val_accuracy: 0.4141\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Define a tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='nn_models',\n",
    "    project_name='disorder_subclass',\n",
    "    seed=_GLOBAL_SEED\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(\n",
    "    train_disorder_subclass_x, train_disorder_subclass_y,\n",
    "    epochs=_TOTAL_EPOCHES,\n",
    "    validation_data=(val_disorder_subclass_x, val_disorder_subclass_y),\n",
    "    # callbacks=[lr_callback]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_disorder_subclass_x, train_disorder_subclass_y, epochs=_TOTAL_EPOCHES, validation_data=(val_disorder_subclass_x, val_disorder_subclass_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 5ms/step - loss: 0.8558 - accuracy: 0.5986\n",
      "113/113 [==============================] - 1s 5ms/step\n",
      "Test Accuracy: 0.5986149311065674\n",
      "Test Recall: 0.5142100075449383\n",
      "Test Mean Squared Error: 1.2764542936288088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_disorder_subclass_x, test_disorder_subclass_y)\n",
    "\n",
    "# Make predictions on the testing dataset\n",
    "test_predictions = model.predict(test_disorder_subclass_x)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate the recall\n",
    "test_recall = recall_score(test_disorder_subclass_y, test_predictions, average='macro')\n",
    "\n",
    "# Calculate the mean squared error\n",
    "test_mse = mean_squared_error(test_disorder_subclass_y, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test Mean Squared Error:\", test_mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genom_prediction_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
