{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the global seed\n",
    "_GLOBAL_SEED = 42\n",
    "random.seed(_GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the training dataset\n",
    "train_df = pd.read_csv('../_Dataset/train_dataset.csv')\n",
    "\n",
    "# Read the test dataset\n",
    "test_df = pd.read_csv('../_Dataset/test_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_disorder_subclass_df = train_df.drop(\"genetic_disorder\", axis=1)\n",
    "test_disorder_subclass_df = test_df.drop(\"genetic_disorder\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_disorder_subclass_x = train_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "train_disorder_subclass_y = train_disorder_subclass_df[\"disorder_subclass\"]\n",
    "\n",
    "test_disorder_subclass_x = test_disorder_subclass_df.drop(\"disorder_subclass\",axis=1)\n",
    "test_disorder_subclass_y = test_disorder_subclass_df[\"disorder_subclass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler and transform the training data\n",
    "train_disorder_subclass_x = scaler.fit_transform(train_disorder_subclass_x)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "test_disorder_subclass_x = scaler.transform(test_disorder_subclass_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "logistic_regression_classifier = LogisticRegression(solver='newton-cg', random_state=_GLOBAL_SEED)\n",
    "kn_classifier = KNeighborsClassifier()\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=_GLOBAL_SEED)\n",
    "gaussian_nb_classifier = GaussianNB()\n",
    "random_forest_classifier = RandomForestClassifier(random_state=_GLOBAL_SEED)\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(random_state=_GLOBAL_SEED)\n",
    "xgb_classifier = XGBClassifier(random_state=_GLOBAL_SEED)\n",
    "lgb_classifier = LGBMClassifier(random_state=_GLOBAL_SEED)\n",
    "svc_classifier = SVC(decision_function_shape='ovo')\n",
    "catb_classifier = CatBoostClassifier(random_state=_GLOBAL_SEED, verbose=False)\n",
    "\n",
    "# Assuming catb_classifier is your CatBoostClassifier\n",
    "catb_classifier = CalibratedClassifierCV(catb_classifier, method='sigmoid', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models on the genetic disorder training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 14437, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -4.886202\n",
      "[LightGBM] [Info] Start training from score -5.287090\n",
      "[LightGBM] [Info] Start training from score -1.741575\n",
      "[LightGBM] [Info] Start training from score -2.375633\n",
      "[LightGBM] [Info] Start training from score -2.699223\n",
      "[LightGBM] [Info] Start training from score -3.439823\n",
      "[LightGBM] [Info] Start training from score -1.350441\n",
      "[LightGBM] [Info] Start training from score -1.522392\n",
      "[LightGBM] [Info] Start training from score -1.948060\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "kn_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "decision_tree_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "gaussian_nb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "random_forest_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "gradient_boosting_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "xgb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "lgb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "svc_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "catb_classifier.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "\n",
    "models_collection = {\n",
    "    'Logistic Regression': logistic_regression_classifier, \n",
    "    'K-Nearest Neighbors': kn_classifier, \n",
    "    'Decision Tree': decision_tree_classifier, \n",
    "    'Gaussian Naive Bayes': gaussian_nb_classifier, \n",
    "    'Random Forest': random_forest_classifier, \n",
    "    'Gradient Boosting': gradient_boosting_classifier, \n",
    "    'XGBoost': xgb_classifier, \n",
    "    'LightGBM': lgb_classifier,\n",
    "    'Support Vector Classifier': svc_classifier,\n",
    "    'CatBoost': catb_classifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_collection.items():\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict(test_disorder_subclass_x)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_disorder_subclass_y, predictions)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = recall_score(test_disorder_subclass_y, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(test_disorder_subclass_y, predictions)\n",
    "    \n",
    "    # Store the results\n",
    "    results[model_name] = {'Accuracy': accuracy, 'Recall': recall, 'Mean Squared Error': mse}\n",
    "    \n",
    "# Sort the results by accuracy\n",
    "sorted_results = sorted(results.items(), key=lambda item: item[1]['Accuracy'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Model: CatBoost\n",
      "Accuracy: 0.6736842105263158\n",
      "Recall: 0.6736842105263158\n",
      "Mean Squared Error: 2.119113573407202\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.6681440443213297\n",
      "Recall: 0.6681440443213297\n",
      "Mean Squared Error: 2.2958448753462606\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: LightGBM\n",
      "Accuracy: 0.66398891966759\n",
      "Recall: 0.66398891966759\n",
      "Mean Squared Error: 2.2601108033240997\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: XGBoost\n",
      "Accuracy: 0.6578947368421053\n",
      "Recall: 0.6578947368421053\n",
      "Mean Squared Error: 2.2922437673130194\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.5961218836565096\n",
      "Recall: 0.5961218836565096\n",
      "Mean Squared Error: 2.7833795013850415\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.5124653739612188\n",
      "Recall: 0.5124653739612188\n",
      "Mean Squared Error: 3.974792243767313\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.4168975069252078\n",
      "Recall: 0.4168975069252078\n",
      "Mean Squared Error: 3.7274238227146816\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Support Vector Classifier\n",
      "Accuracy: 0.3941828254847645\n",
      "Recall: 0.3941828254847645\n",
      "Mean Squared Error: 3.8761772853185597\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: Gaussian Naive Bayes\n",
      "Accuracy: 0.38864265927977837\n",
      "Recall: 0.38864265927977837\n",
      "Mean Squared Error: 4.073407202216067\n",
      "---------------------------\n",
      "---------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.3038781163434903\n",
      "Recall: 0.3038781163434903\n",
      "Mean Squared Error: 5.983102493074792\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the sorted results\n",
    "for model_name, metrics in sorted_results:\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the top 4 models to create an ensemle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 720\n",
      "[LightGBM] [Info] Number of data points in the train set: 14437, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -4.886202\n",
      "[LightGBM] [Info] Start training from score -5.287090\n",
      "[LightGBM] [Info] Start training from score -1.741575\n",
      "[LightGBM] [Info] Start training from score -2.375633\n",
      "[LightGBM] [Info] Start training from score -2.699223\n",
      "[LightGBM] [Info] Start training from score -3.439823\n",
      "[LightGBM] [Info] Start training from score -1.350441\n",
      "[LightGBM] [Info] Start training from score -1.522392\n",
      "[LightGBM] [Info] Start training from score -1.948060\n",
      "Ensemble accuracy: 0.6759002770083102\n",
      "Ensemble recall: 0.6759002770083102\n",
      "Ensemble mean squared error: 2.0759002770083104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Get the top 4 models\n",
    "top_4_models = sorted_results[:4]\n",
    "\n",
    "# Create a list of tuples where each tuple contains a model name and an estimator\n",
    "estimators = [(model_name, models_collection[model_name]) for model_name, _ in top_4_models]\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble.fit(train_disorder_subclass_x, train_disorder_subclass_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "ensemble_predictions = ensemble.predict(test_disorder_subclass_x)\n",
    "\n",
    "# Calculate and print the accuracy of the ensemble model\n",
    "ensemble_accuracy = accuracy_score(test_disorder_subclass_y, ensemble_predictions)\n",
    "\n",
    "# Calculate recall\n",
    "ensemble_recall = recall_score(test_disorder_subclass_y, ensemble_predictions, average='weighted')\n",
    "\n",
    "# Calculate mean squared error\n",
    "ensemble_mse = mean_squared_error(test_disorder_subclass_y, ensemble_predictions)\n",
    "\n",
    "print(f\"Ensemble accuracy: {ensemble_accuracy}\")\n",
    "print(f\"Ensemble recall: {ensemble_recall}\")\n",
    "print(f\"Ensemble mean squared error: {ensemble_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genom_prediction_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
